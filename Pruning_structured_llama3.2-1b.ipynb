{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT0FAsUnncFY"
   },
   "source": [
    "<div> \n",
    "    <h2>Pruning Llama 3.2.</h2>  \n",
    "</div>\n",
    "\n",
    "* Pruning\n",
    "* Structured pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEwxZCVsoIau"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The pruning process was based on selecting neurons from the model's MLP layers that have the least importance using the L1 norm, assuming these contributed the least to the model's output.\n",
    "\n",
    "However, by ignoring the model's structure, some problems arose, which are addressed in this notebook, by taking the actions:\n",
    "\n",
    "* Consider the GLU (Gated Linear Unit) structure of the MLP layers.\n",
    "* Use a neuron selection method that is compatible with the GLU structure.\n",
    "\n",
    "In this notebook, we focus on explaining the modifications made to the pruning process that have successfully allowed us to create a smaller model while retaining almost all the functionalities of the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQIxAOPZtPBN"
   },
   "source": [
    "#Install libraries & Configure variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zHApVm41HWq",
    "outputId": "015adf1f-b283-41f0-c204-325ed906bd47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from dotenv) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install dotenv\n",
    "!pip install -q torch\n",
    "!pip install -q datasets\n",
    "!pip install -q sentencepiece  # Required for LLaMA tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GJNgRj4M187E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbIyUlXEtbqs",
    "outputId": "661e4946-0053-47a6-b91e-e55d5d12eaa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM-QwxyKw-YG"
   },
   "source": [
    "#Download model and explore structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration from environment variables\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "model_name = os.getenv('MODEL_NAME', 'meta-llama/Llama-3.2-1B')\n",
    "\n",
    "# Check if HF_TOKEN is provided\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file.\")\n",
    "\n",
    "# Load model and tokenizer with authentication token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    token=hf_token\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9UpMD4Hw2MWg"
   },
   "outputs": [],
   "source": [
    "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        do_sample=False,          # Disable sampling\n",
    "        num_beams=5,              # Use beam search\n",
    "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
    "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4muyx_8M5OAu"
   },
   "source": [
    "## studying the model structure\n",
    "As demonstrated in the [previous notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb), studying the structure of the model that will undergo pruning is crucial.\n",
    "\n",
    "In this notebook, we’re going to fine-tune the pruning process for the Llama3.2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5Hs4oQ4B7Z0",
    "outputId": "61ccc5f9-26a9-4b52-ee7b-9a5c487a00dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPMslK3QCAb1"
   },
   "source": [
    "\n",
    "An MLP block typically consists of layers that scale the data to larger dimensions and others that return it to its original size.\n",
    "\n",
    "In the MLP block of the model, we find two projection layers: `gat_proj` and `down_proj`, both scaling from 2048 to 8192. The purpose of having two layers projecting to the same intermediate size might be related to gating mechanisms. A gating mechanism selectively controls information flow in neural networks by using learned weights to \"gate\" or filter inputs.\n",
    "\n",
    "However, to truly understand how these layers function, we’d need to refer to the model's documentation or even the source code. But, this structure usually indicates, at least, I haven't encountered a case where it doesn't, that the layers performing the upsizing work in pairs, and they cannot be treated as independent linear layers.\n",
    "\n",
    "In other words, any operation we apply to one layer must be replicated in the other. Most importantly, when identifying which neurons have more or less importance, we can't evaluate the neurons of a single layer in isolation; we need to treat them as pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alKH3QH64WFL",
    "outputId": "839eae37-b7e4-436d-ad91-c552701200de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n"
     ]
    }
   ],
   "source": [
    "# Test the original model\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated = get_output(prompt)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8WR96iwq2XYH"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kph43oObnet7",
    "outputId": "c77780b4-b42e-4033-96e0-68e345d288cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 1235814400\n"
     ]
    }
   ],
   "source": [
    "original_param_count = count_parameters(model)\n",
    "print(f\"Original model parameters: {original_param_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK9NwmBWnkSP"
   },
   "source": [
    "#Pruning the Model.\n",
    "##Support pruning functions.\n",
    "###Compute neuron importance functions.\n",
    "\n",
    "Here are three functions I used to calculate neuron importance, allowing us to decide which ones to eliminate.\n",
    "\n",
    "All three functions take into account that the layers should be treated as pairs, considering both layers to calculate neuron importance.\n",
    "\n",
    "The results obtained with each function have been quite different:\n",
    "\n",
    "* **Product of Norms**: Paris is the capital of of of of the of the the the the to to to from to from from from to to from to\n",
    "France France France France France France France France France France France\n",
    "France France France France France\n",
    "\n",
    "* **Variance of weights**: Paris is the capital of the French Republic. It is also a...\n",
    "Paris is the capital of the French Republic. It is also a\n",
    "Germany is the German Republic. It is also a\n",
    "of the Austrian Republic. It is also a\n",
    "\n",
    "* **Maximum absolute weight**: Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are a few things you should not miss while you\n",
    "\n",
    "* **Base model**: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n",
    "\n",
    "It seems clear that the **Absolute Maximum** calculation has worked the best. I'd say the other methods for selecting neurons to remove have severely degraded the model, or at least eliminated a significant portion of the base model's capabilities.\n",
    "\n",
    "*I’m leaving the others in the notebook purely as an exercise.*\n",
    "\n",
    "The **Maximum Absolute Weight** method works better because it directly identifies the most influential neurons based on the magnitude of their connections. These neurons are likely responsible for key decisions, making the model more accurate after pruning. The Variance of Weights method, while useful in some contexts, can retain neurons that may not contribute significantly to the task, leading to less coherent model outputs.\n",
    "\n",
    "However, we shouldn’t fall into the trap of assuming that this neuron selection method will work best across all model structures. It works well with Llama models, and this may be due to several factors:\n",
    "\n",
    "* The relatively large projection from 2048 to 8192.\n",
    "* The use of a GLU structure.\n",
    "* The type of activation function used.\n",
    "\n",
    "So, if we use a model from another family, like Gemma or Mistral, the neuron selection method might need to be entirely different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fflgE2Y8_eZF"
   },
   "outputs": [],
   "source": [
    "#****DISCARTED****\n",
    "#Product of Norms:\n",
    "#Since the GLU multiplies the outputs of gate_proj and up_proj,\n",
    "#we can compute the product of their weight norms to better represent the\n",
    "#importance of the neuron pair\n",
    "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
    "\n",
    "    gate_norms = torch.norm(gate_weight, p=1, dim=1)\n",
    "    up_norms = torch.norm(up_weight, p=1, dim=1)\n",
    "    importance_scores = gate_norms * up_norms\n",
    "    return importance_scores\n",
    "#sample response: Paris is the capital of of of of the of the the the the to to to from to from from from to to from to\n",
    "#France France France France France France France France France France France\n",
    "#France France France France France\n",
    "#All All\n",
    "#All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_mMGhTtD4PRX"
   },
   "outputs": [],
   "source": [
    "#****DISCARTED****\n",
    "#Variance of Weights\n",
    "#Neurons with higher weight variance may contribute more to the model's output.\n",
    "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
    "    gate_variance = torch.var(gate_weight, dim=1)\n",
    "    up_variance = torch.var(up_weight, dim=1)\n",
    "    importance_scores = gate_variance + up_variance\n",
    "    return importance_scores\n",
    "#sample response: Paris is the capital of the French Republic. It is also a...\n",
    "#Paris is the capital of the French Republic. It is also a\n",
    "#Germany is the German Republic. It is also a\n",
    "#of the Austrian Republic. It is also a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Seyqaquj7mQA"
   },
   "outputs": [],
   "source": [
    "#****SELECTED****\n",
    "#Maximum Absolute Weight:\n",
    "#The maximum absolute weight in a neuron might indicate its significance.\n",
    "\n",
    "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
    "  \"\"\"\n",
    "  compute neuron pair importance scores (Maximum Absolute Weight)\n",
    "\n",
    "  Args:\n",
    "  - gate_weight: Weight matrix from the gate_proj layer.\n",
    "  - up_weight: Weight matrix from the up_weight layer.\n",
    "\n",
    "  Returns:\n",
    "  - importance_scores: Importance scores for each neuron pair.\n",
    "  \"\"\"\n",
    "\n",
    "  gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
    "  up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
    "  importance_scores = gate_max_abs + up_max_abs\n",
    "  return importance_scores\n",
    "\n",
    "#response: Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are a few things you should not miss while you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NX9Boph94RWA"
   },
   "outputs": [],
   "source": [
    "#Prunes a specific percentatge of neurons from the MLP (feed forward layers).\n",
    "def prune_neuron_pairs(mlp, prune_percent):\n",
    "    \"\"\"\n",
    "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
    "    layers removing the least important neurons.\n",
    "\n",
    "    Args:\n",
    "    - mlp: Layers to prune.\n",
    "    - prune_percent: Percentage of neurons to prune.\n",
    "\n",
    "    Returns:\n",
    "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
    "    - k: New intermediate size.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract the weights from the MLP layers\n",
    "    #  these weights are used to calculate each neuron's\n",
    "    #  importance score in the next step.\n",
    "    gate_weight = mlp.gate_proj.weight.data.float()\n",
    "    up_weight = mlp.up_proj.weight.data.float()\n",
    "\n",
    "    #Compute importance stores. Neurons with higher importance scores\n",
    "    # are considered more important and less likely to be pruned.\n",
    "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
    "\n",
    "    #Store the original number of neurons in the intermediate layer.\n",
    "    original_intermediate_size = gate_weight.size(0)\n",
    "    #Computes the number of neurons to prune.\n",
    "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
    "    #Calculate the number of neurons to keep. The new intermediate size.\n",
    "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
    "\n",
    "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
    "\n",
    "    #Select the neuros to keep, by obtaining the indices to keep.\n",
    "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
    "    indices_to_keep = indices_to_keep.sort().values\n",
    "\n",
    "    #create the new layers\n",
    "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
    "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
    "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
    "\n",
    "    #copy weights to the new layers.\n",
    "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
    "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
    "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
    "\n",
    "    #return new layers and intermediate size.\n",
    "    return new_gate_proj, new_up_proj, new_down_proj, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT0v_RpeST87"
   },
   "source": [
    "# Prune Loop\n",
    "The update_model function iterates through the blocks within the model's Transformer structure. This structure consists of multiple `LlamaDecoderLayer` blocks, and each of these blocks contains a pair of `LlamaSdpaAttention` and `LlamaMLP` components. The latter contains the MLP layers that will be the target of the pruning process.\n",
    "```\n",
    "(layers): ModuleList(\n",
    "      (0-15): 16 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaSdpaAttention(\n",
    "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "      )\n",
    "  )    \n",
    "```\n",
    "The layers that will undergo the removal of neurons identified as less useful are:\n",
    "```\n",
    "(gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "(up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "(down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "```\n",
    "The neurons are removed in the `prune_neurons` function based on the values returned by `compute_neuron_pair_importance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FxJEWg1X3j0m"
   },
   "outputs": [],
   "source": [
    "#Iterates throught the model layers and applies pruning.\n",
    "def update_model(model, prune_percent):\n",
    "    \"\"\"\n",
    "    It modifies each mlp layer present in model, to retain only the most\n",
    "    important neurons. Creating new smaller versions of each layer pruned.\n",
    "\n",
    "    Args:\n",
    "    - model: Model to prune.\n",
    "    - prune_percent: Percentage of neurons to prune.\n",
    "\n",
    "    Returns:\n",
    "    - model: New pruned model.\n",
    "    \"\"\"\n",
    "    new_intermediate_size = None\n",
    "\n",
    "    #loop for each model layer.\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
    "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
    "        # by accesing layer.mlp.\n",
    "        mlp = layer.mlp\n",
    "\n",
    "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
    "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
    "\n",
    "        #Replace the Origiginal Layers with Pruned Layers.\n",
    "        mlp.gate_proj = new_gate_proj\n",
    "        mlp.up_proj = new_up_proj\n",
    "        mlp.down_proj = new_down_proj\n",
    "\n",
    "        #new_intermediate_size only needs to be set once\n",
    "        if new_intermediate_size is None:\n",
    "            new_intermediate_size = new_size\n",
    "\n",
    "    #Update the model config file.\n",
    "    model.config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtHtSbRmS267"
   },
   "source": [
    "## Obtain & test the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NIUnFU5R3n42"
   },
   "outputs": [],
   "source": [
    "prune_percent = 0.2  # Prune 20% of neurons\n",
    "model = update_model(model, prune_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdJUkfWI3qMM",
    "outputId": "b23e2414-25f4-4c96-eb99-afecd48eea20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model parameters: 1074792448\n",
      "Reduction in parameters: 161021952\n",
      "Percentage of weight savings: 13.03%\n"
     ]
    }
   ],
   "source": [
    "# Recalculate the number of parameters\n",
    "pruned_param_count = count_parameters(model)\n",
    "reduction_in_params = original_param_count - pruned_param_count\n",
    "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
    "\n",
    "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
    "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
    "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvj-iIsO5M6U",
    "outputId": "e0d470bf-c7cd-4fbf-ec9f-4381e154e9f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after pruning: Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are some things you\n"
     ]
    }
   ],
   "source": [
    "# Test the pruned model\n",
    "generated = get_output(prompt, model, tokenizer)\n",
    "print(f\"Generated text after pruning: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGzXMQrVTULv"
   },
   "source": [
    "The result is slightly different from what the original model produced, but it’s still a fairly accurate response.\n",
    "\n",
    "In contrast to the model created in notebook: [6_2_pruning_structured_llama3.2-1b_KO.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb) where the pruned Llama model lost almost all its utility, the model in this notebook retains a good portion of its knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDQrSrf-VCyI"
   },
   "source": [
    "Looking at the model’s new structure, we can see that the `gate_proj` and `up_proj` layers have had their `out_features` reduced to 6554 from 8192. Consequently, the `down_proj` layer has its `in_features` adjusted to match the new size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATAiqZW30NYN",
    "outputId": "a124260f-8192-477b-9e16-f0e7bbe96df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
      "          (down_proj): Linear(in_features=6554, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6qEmvooZycx"
   },
   "source": [
    "#Upload the model to HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2Ll_kqe5QzO",
    "outputId": "f20dc674-5c74-40d2-ec9d-184701643707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved to ./pruned20-llama-1b-st\n"
     ]
    }
   ],
   "source": [
    "new_model_name = 'pruned20-llama-1b-st'\n",
    "output_dir = './'+new_model_name\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Pruned model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3LjjsGZV5ZHJ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e480b500584acd88c21fcbb7f47e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/golongson/pruned20-llama-1b-st/commit/366a7b0ebd36540be36ee0603f9524234b2cf65a', commit_message='Upload LlamaForCausalLM', commit_description='', oid='366a7b0ebd36540be36ee0603f9524234b2cf65a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/golongson/pruned20-llama-1b-st', endpoint='https://huggingface.co', repo_type='model', repo_id='golongson/pruned20-llama-1b-st'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model to your Hugging Face repository\n",
    "\n",
    "model.push_to_hub(new_model_name, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6xNN-aYa5h9B"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518a1b2c36844d4b9ac830d625f811ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e876c0d4e4db433ca8e08223ebe58040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/golongson/pruned20-llama-1b-st/commit/de6e8b9a5cd7b52404a06c4775d9553f278269fd', commit_message='Upload tokenizer', commit_description='', oid='de6e8b9a5cd7b52404a06c4775d9553f278269fd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/golongson/pruned20-llama-1b-st', endpoint='https://huggingface.co', repo_type='model', repo_id='golongson/pruned20-llama-1b-st'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(new_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdKFR5Ju23kI"
   },
   "source": [
    "#Evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UM2pkqFAYEe"
   },
   "source": [
    "In this section, we'll take a look at some standard evaluations in the world of Large Language Models using the lm-evaluation library from EleutherAI.\n",
    "\n",
    "Specifically, we'll use LAMBADA and BoolQ. Since the pruning performed could be considered structural—that is, it affects the model's overall structure without a specific target—I’ve chosen two rather different evaluation tasks.\n",
    "\n",
    "I want to remind you that the goal of this notebook is to demonstrate the pruning process, so I won’t be doing a comprehensive study of how it impacts performance; that will be saved for a future article. Additionally, these models are designed to be fine-tuned before being used.\n",
    "\n",
    "However, I believe that seeing how pruning impacts model performance can help illustrate the pruning process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM-Eval Code Explanation\n",
    "\n",
    "- **Installation**: Installs `accelerate` library first (required for device mapping), then `lm-eval` library for model evaluation\n",
    "\n",
    "- **Environment Setup**: Loads Hugging Face token from `.env` file using `dotenv` for accessing private/gated models\n",
    "\n",
    "- **evaluate_hf_model() Function**: \n",
    "  - Takes model name, evaluation tasks, few-shot examples count, device, and optional HF token as parameters\n",
    "  - Builds model arguments string with device mapping (`device_map=auto` for automatic GPU allocation)\n",
    "  - Adds authentication token to model args if provided\n",
    "\n",
    "- **Model Evaluation**: Uses `evaluator.simple_evaluate()` to run specified tasks (lambada, boolq) on the Hugging Face model with 10 bootstrap iterations\n",
    "\n",
    "- **Device Handling**: Uses `device_map=auto` instead of hardcoded CUDA device for better compatibility with the accelerate library\n",
    "\n",
    "- **Return Value**: Extracts and returns evaluation metrics from the results dictionary\n",
    "\n",
    "- **Usage**: Evaluates \"meta-llama/Llama-3.2-1B\" model on language understanding tasks (lambada for completion, boolq for boolean questions)\n",
    "\n",
    "  ## What are LAMBADA and BoolQ?\n",
    "\n",
    "### LAMBADA\n",
    "- **Task Type**: Word prediction task that evaluates computational models for text understanding \n",
    "- **Dataset**: Collection of narrative passages where humans can guess the last word when exposed to the whole passage \n",
    "- **Purpose**: Tests the model's ability to understand broad discourse context and predict missing words in stories\n",
    "- **Example**: Given a story passage, the model must predict the final word that logically completes the narrative\n",
    "\n",
    "### BoolQ  \n",
    "- **Task Type**: Question answering dataset for yes/no questions containing 15,942 examples \n",
    "- **Dataset**: Naturally occurring questions generated in unprompted and unconstrained settings \n",
    "- **Format**: Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context \n",
    "- **Purpose**: Tests the model's ability to answer True/False questions based on reading comprehension \n",
    "- **Example**: Given a passage about a topic and a yes/no question, the model must determine if the answer is True or False\n",
    "\n",
    "### Why These Tasks Matter\n",
    "- **LAMBADA**: Measures long-range context understanding and narrative comprehension\n",
    "- **BoolQ**: Evaluates reading comprehension and logical reasoning for binary classification tasks\n",
    "- **Combined**: Together they test both generative (word prediction) and discriminative (classification) language understanding capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746f8ef7f35e40feb8798db5796525b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd14c733d83455b83e8612658d59d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00002.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f847f6410943bcb644bbaee57e6768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00001-of-00002.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fe550f0e404f038f28d870357be9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7843f7af5ab94516a148d3d9e5b698d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae244406f9dd4ed29596567b2e2e1b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cc737f298c4c708bf1b0b0ddcdd0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb317e175ea34819930cc3005871dd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf1cc45925046578d9a57bbf3bb1c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e877cb6c697b4a5c812dbdca2e4e1f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "default/test/default.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15473f2e7744d9da3a241a8cf30dac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c945f45064e7fa28ca08f7e76007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89500369b37a48748fbc10ddcab6ebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boolq/train-00000-of-00001.parquet:   0%|          | 0.00/3.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0636bd69b030491db949c01db0576ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boolq/validation-00000-of-00001.parquet:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbdee3e015f4a729ba5539ad7891492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "boolq/test-00000-of-00001.parquet:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eec4cdcaa2a4617935ae25112ba2342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e51c7f703e84be183edd08ed7ebc04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c7ea5eaab4bc1803a43db5e7267a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3270/3270 [00:01<00:00, 2721.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5153/5153 [00:06<00:00, 808.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5153/5153 [00:06<00:00, 794.85it/s]\n",
      "Running loglikelihood requests: 100%|█████████████████████████████████████████████| 16846/16846 [20:25<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 155.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 205.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies in correct order\n",
    "!pip install -q accelerate\n",
    "!pip install -q lm-eval\n",
    "\n",
    "# For models requiring authentication\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load HF token if needed\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "from lm_eval import evaluator, tasks, models\n",
    "\n",
    "def evaluate_hf_model(model_name, tasks=['arc_easy'], num_fewshot=0, device=\"auto\", hf_token=None):\n",
    "    \"\"\"\n",
    "    It calls the evaluator to evaluate a model available on Hugging Face.\n",
    "    Args:\n",
    "    - model_name: The model name in Hugging Face.\n",
    "    - tasks: Tasks to evaluate.\n",
    "    - num_fewshot: Number of examples of few-shot learning\n",
    "    - device: Device to use (\"auto\", \"cuda\", \"cpu\")\n",
    "    - hf_token: Hugging Face token for private models\n",
    "    Returns:\n",
    "    - metrics.\n",
    "    \"\"\"\n",
    "    # Build model_args string\n",
    "    if device == \"auto\":\n",
    "        model_args = f\"pretrained={model_name},device_map=auto\"\n",
    "    else:\n",
    "        model_args = f\"pretrained={model_name},device={device}\"\n",
    "    \n",
    "    # Add token if provided\n",
    "    if hf_token:\n",
    "        model_args += f\",token={hf_token}\"\n",
    "    \n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf\",\n",
    "        model_args=model_args,\n",
    "        tasks=tasks,\n",
    "        num_fewshot=num_fewshot,\n",
    "        limit=None,\n",
    "        bootstrap_iters=10\n",
    "    )\n",
    "    metrics = results.get('results', {})\n",
    "    return metrics\n",
    "\n",
    "# Select tasks to evaluate\n",
    "tasks = ['lambada', 'boolq']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics_base = evaluate_hf_model(\"meta-llama/Llama-3.2-1B\", tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "w-g3vyPN3VZp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boolq': {'alias': 'boolq',\n",
       "  'acc,none': 0.637308868501529,\n",
       "  'acc_stderr,none': 0.008408838061823177},\n",
       " 'lambada_openai': {'alias': 'lambada_openai',\n",
       "  'perplexity,none': 5.747471606969041,\n",
       "  'perplexity_stderr,none': 0.19350717486069613,\n",
       "  'acc,none': 0.6198331069280031,\n",
       "  'acc_stderr,none': 0.006762956659647619},\n",
       " 'lambada_standard': {'alias': 'lambada_standard',\n",
       "  'perplexity,none': 8.673077754353926,\n",
       "  'perplexity_stderr,none': 0.3809304515805616,\n",
       "  'acc,none': 0.5315350281389482,\n",
       "  'acc_stderr,none': 0.006952109107344538}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "aN5KeIkQ15gM",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e583c1c48dcf4829bb51d088616368d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c24fbf68bc74672b50a161cad13cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070403f2a2de4d39a96c11b52f9beec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee88da7aee0c4c3db40272ecde10967b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977c5cc3d282466a8d795feae8694737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-09-22T03:22:41.881817Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://transfer.xethub.hf.co/xorbs/default/c8ea8d11668e1db0fcc30e5c2f2910e8ff243dbdbb4ec9abd4a1460ddb00a2cf?X-Xet-Signed-Range=bytes%3D26237298-26246930&X-Xet-Session-Id=01K5QQ2HX3EGNYGE60FMR3F82R&Expires=1758514922&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC9jOGVhOGQxMTY2OGUxZGIwZmNjMzBlNWMyZjI5MTBlOGZmMjQzZGJkYmI0ZWM5YWJkNGExNDYwZGRiMDBhMmNmP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDI2MjM3Mjk4LTI2MjQ2OTMwJlgtWGV0LVNlc3Npb24tSWQ9MDFLNVFRMkhYM0VHTllHRTYwRk1SM0Y4MlIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTg1MTQ5MjJ9fX1dfQ__&Signature=jij2SfYPgIwxFLz9YKv3NSAOQAUyzmCM-4nzANdJZ8FlX10Ry7MZYmdrIrgodUAGMomEk30KlB-q~5QDDG7PfF81OGxrublrrDy3vfUPDwRSt3fy1~olLlp-t1E3NHJm4QnfoCMBth31~xR50~V-AahdL9BWt1FhnqBQwKhKnUC2N2bougKCPaDi8JCauDp5LPNt36exjbkkAMnCGrON7E~glLE0AXY2pcVbxl901uVrYizv1rTRdF28WXwv~pNObZNsCyD202wARNQ1hjaJHzbJj2ITIuEr9gUr9yMdsaZnEmW8j140RM9dd~8iH5kJwXZcz1x59FZc0c-WltQ8IA__&Key-Pair-Id=K2L8F4GPSG1IFC\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(Io, Os { code: 110, kind: TimedOut, message: \"Connection timed out\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:233\n",
      "\n",
      "  \u001b[2m2025-09-22T03:22:41.888095Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 2.275083673s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dd2b86abae4dca8cbb5ea6476bbcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: boolq] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "[Task: boolq] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "Overwriting default num_fewshot of boolq from None to 0\n",
      "Overwriting default num_fewshot of lambada_openai from None to 0\n",
      "Overwriting default num_fewshot of lambada_standard from None to 0\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3270/3270 [00:01<00:00, 2693.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5153/5153 [00:06<00:00, 772.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5153/5153 [00:06<00:00, 776.05it/s]\n",
      "Running loglikelihood requests: 100%|█████████████████████████████████████████████| 16846/16846 [15:58<00:00, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 140.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 230.58it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics_pruned = evaluate_hf_model(\"oopere/pruned40-llama-1b\", tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bboB7uU39l_Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boolq': {'alias': 'boolq',\n",
       "  'acc,none': 0.6226299694189602,\n",
       "  'acc_stderr,none': 0.008477957863309994},\n",
       " 'lambada_openai': {'alias': 'lambada_openai',\n",
       "  'perplexity,none': 90.35517481351984,\n",
       "  'perplexity_stderr,none': 5.0287605671467475,\n",
       "  'acc,none': 0.29477973995730644,\n",
       "  'acc_stderr,none': 0.006352187060216686},\n",
       " 'lambada_standard': {'alias': 'lambada_standard',\n",
       "  'perplexity,none': 171.38470785943537,\n",
       "  'perplexity_stderr,none': 8.343271835349864,\n",
       "  'acc,none': 0.24335338637686785,\n",
       "  'acc_stderr,none': 0.005978294650852376}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWNOHoVdpgcP"
   },
   "source": [
    "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/lambada_BooQ_Accuracy.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c1FodzbrwMv"
   },
   "source": [
    "As we can see, the effect of pruning has been somewhat asymmetrical. The tasks evaluated by the BoolQ test haven’t experienced significant degradation—only about a 2% drop for a model that lost 35% of its weight.\n",
    "\n",
    "In contrast, the impact on the Lambada test has been remarkable, with a drop in accuracy of over 50%.\n",
    "\n",
    "This indicates that the model retains much of its comprehension ability but struggles with tests requiring more open-ended generation.\n",
    "\n",
    "BoolQ simply presents the model with a text and a question to be answered with Yes/No. It’s a test focused on measuring the model’s ability to understand relationships within the input text.\n",
    "\n",
    "Lambada, on the other hand, asks the model to guess the last word of a paragraph, a complex task where the final word tests the model’s capability in complex language modeling.\n",
    "\n",
    "These results are consistent with the functionality of the MLP layers that were pruned.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
