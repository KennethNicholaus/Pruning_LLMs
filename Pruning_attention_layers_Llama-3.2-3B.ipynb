{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV5KPbfseL8S"
   },
   "source": [
    "<div>\n",
    "    <h2>Pruning Attention Layers Models: meta-llama/Llama-3.2-3B/h2>\n",
    "    <h3>Not All Attention is needed</h3>\n",
    "</div>\n",
    "\n",
    "* Pruning\n",
    "* Attention based on cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwYeKwswnkTG"
   },
   "source": [
    "# Install libraries & Configure variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PblPrYCiYTl8",
    "outputId": "5374f0f7-2a56-4ce4-ea9d-d6e4fcaacfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: hf_xet in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (1.1.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch==2.6.0\n",
    "!pip install dotenv\n",
    "!pip install -q torchvision==0.21.0\n",
    "!pip install -q transformers==4.51.3\n",
    "!pip install -q datasets==3.6.0\n",
    "!pip install -q lm-eval==0.4.8\n",
    "\n",
    "!pip install hf_xet #To speed up downloads from HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6qN0mu6IHqpy"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8iRr5iapy5q"
   },
   "source": [
    "# Download the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9IFTYa9P6Zy",
    "outputId": "06eea4ec-58bd-47dc-dab1-7a307d1cc09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sbnDW_tZRTGp"
   },
   "outputs": [],
   "source": [
    "# #model_name = 'meta-llama/Llama-3.2-1B'\n",
    "# model_name = 'meta-llama/Llama-3.2-3B'\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# #tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e00782265b4400811c8c5c2e53f634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca79554b4924e52847ea52ade5fb80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f2092ca7fc4d61bf0810aa19cc32e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66a18c551224d418d7cdcf2886dbe20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b54cbac869c45b28f504fcb9afd5f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeaba2f5088496ebf1db60d094b71d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bac9868b0a347d9950dd590d9545e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5594bd84d1d4337a5e517ed0bc1724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42a716d149b45258264141be24eb3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589c96d20e374846b5a2d4f6a05b7248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B loaded successfully\n",
      "Tokenizer for meta-llama/Llama-3.2-3B loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the HF_TOKEN from environment\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Define your model name\n",
    "model_name = 'meta-llama/Llama-3.2-3B'\n",
    "\n",
    "# Load the model with the HF_TOKEN for authentication\n",
    "device = \"cuda\"  # or \"cpu\" depending on your setup\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token).to(device)\n",
    "\n",
    "# Load the tokenizer with the HF_TOKEN for authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "\n",
    "# Optionally, print the model and tokenizer to verify successful loading\n",
    "print(f\"Model: {model_name} loaded successfully\")\n",
    "print(f\"Tokenizer for {model_name} loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0ifeungrbHw"
   },
   "source": [
    "## Study the structure.\n",
    "* Llama-3.2-1B\n",
    "```\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(128256, 2048)\n",
    "    (layers): ModuleList(\n",
    "      (0-15): 16 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaSdpaAttention(\n",
    "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
    "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
    "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
    "    (rotary_emb): LlamaRotaryEmbedding()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "The model follows the typical structure of modern Llama models, consisting of blocks made up of an Attention layer and an MLP layer with a GLU structure.\n",
    "\n",
    "> If you want to see an example of how to perform pruning on the MLP layers of the model, you can check out the notebook:[Pruning Llama 3.2.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) y leer el paper [Exploring GLU expansion ratios: Structured pruning in Llama-3.2 models](https://osf.io/preprints/osf/qgxea)\n",
    "\n",
    "\n",
    "\n",
    "Since the layers form a block, the attention layer cannot be removed without also removing the accompanying MLP layer. For this reason, the decision was made to deactivate their execution during inference.\n",
    "\n",
    "The 1B model has 16 layers, as shown in the structure above, while the 3B model has 28 layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF4cHUb_rICs"
   },
   "source": [
    "# Inference function & Test Base Model\n",
    "\n",
    "The `get_output` function is designed to generate text  and measure the time taken for different stages of the generation process.\n",
    "\n",
    "It provides insights into the performance of the model and can be used to evaluate the efficiency of text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Measuring Time for Tokenization, Generation, and Decoding in LLM\n",
    "\n",
    "- **Function Purpose**:  \n",
    "  The `get_output` function measures the time taken for tokenization, text generation, and decoding in a language model (LLM) for a given prompt.\n",
    "\n",
    "- **Inputs**:  \n",
    "  - `prompt`: Text input to generate a response.\n",
    "  - `model`: Pre-trained language model.\n",
    "  - `tokenizer`: Tokenizer for encoding/decoding text.\n",
    "  - `num_runs`: Number of times to repeat the process (default: 1).\n",
    "  - `max_length`: Maximum length of generated text (default: 50).\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Tokenization**:  \n",
    "     - The prompt is tokenized using the tokenizer and moved to the specified device (CPU/GPU).\n",
    "     - Time taken for tokenization is measured.\n",
    "  2. **Text Generation**:  \n",
    "     - The model generates text using the input token IDs with specific settings (e.g., beam search, no sampling, no repetition of 2-grams).\n",
    "     - Time taken for generation is measured.\n",
    "  3. **Decoding**:  \n",
    "     - The generated token IDs are decoded back into readable text.\n",
    "     - Time taken for decoding is measured.\n",
    "\n",
    "- **Output**:\n",
    "  - For each run, the time for tokenization, generation, and decoding is printed.\n",
    "  - After multiple runs, the average total time is displayed.\n",
    "  - The generated text from the model is returned.\n",
    "\n",
    "- **Performance Metrics**:  \n",
    "  - `Tokenization time`, `Generation time`, and `Decoding time` are printed in milliseconds.\n",
    "  - Total time for all steps is calculated and displayed.\n",
    "\n",
    "- **Use Case**:  \n",
    "  This function is useful for benchmarking the performance of the model, especially when evaluating time spent on each stage of text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2igvy4z6rGgy"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n",
    "    total_time = 0\n",
    "    generated_outputs = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Tokenization time\n",
    "        token_start = time.time()\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        token_time = time.time() - token_start\n",
    "\n",
    "        # Generation time\n",
    "        gen_start = time.time()\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            do_sample=False,  # Disable sampling\n",
    "            num_beams=5,      # Use beam search\n",
    "            early_stopping=True,  # Stop when end-of-sequence token is generated\n",
    "            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n",
    "        )\n",
    "        gen_time = time.time() - gen_start\n",
    "\n",
    "        # Decoding time\n",
    "        decode_start = time.time()\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        decode_time = time.time() - decode_start\n",
    "\n",
    "        # Total time for this run\n",
    "        total_time += time.time() - start_time\n",
    "        generated_outputs.append(generated)\n",
    "\n",
    "        if num_runs > 1:\n",
    "            print(f\"\\nRun {run + 1}:\")\n",
    "        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n",
    "        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n",
    "        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n",
    "        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n",
    "\n",
    "    if num_runs > 1:\n",
    "        avg_time = total_time / num_runs\n",
    "        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n",
    "\n",
    "    return generated_outputs[0] if num_runs == 1 else generated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lH7cotAxrhO3",
    "outputId": "a836bc52-d0f6-418c-9c09-06866217807a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1:\n",
      "Tokenization time: 305.41 ms\n",
      "Generation time: 45370.79 ms\n",
      "Decoding time: 0.36 ms\n",
      "Total time: 45676.72 ms\n",
      "\n",
      "Run 2:\n",
      "Tokenization time: 0.79 ms\n",
      "Generation time: 42755.08 ms\n",
      "Decoding time: 0.22 ms\n",
      "Total time: 42756.24 ms\n",
      "\n",
      "Average time over 2 runs: 44216.33 ms\n",
      "Generated text: ['Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city', 'Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city']\n"
     ]
    }
   ],
   "source": [
    "# Test the original model\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated = get_output(prompt, num_runs=2)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo4IjOYGry0W"
   },
   "source": [
    "The text generation of the original model, as expected, works perfectly and returns a correct and meaningful sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bLN1_gLdt7Rx"
   },
   "outputs": [],
   "source": [
    "model.to(\"cpu\")               # actual data moves ↙\n",
    "torch.cuda.empty_cache()      # allocator drops cached blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjgf4WA_vF3B"
   },
   "source": [
    "# Pruning the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TQDCkoL6RW3C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdQmEud3zqSp"
   },
   "source": [
    "This function `measure_unpruned_layer_importances` is designed to calculate importance scores for the attention layers in a model.\n",
    "\n",
    "The basic idea is: if a layer's output is very similar to its input, it might not be doing much important work and could be a candidate for pruning. To check the difference I'm using the cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Measuring Importance of Unpruned Layers in a Pruned Model\n",
    "\n",
    "- **Function Purpose**:  \n",
    "  The `measure_unpruned_layer_importances` function measures the importance of layers in a pruned model. It calculates importance scores for unpruned (non-bypassed) attention layers by comparing the cosine similarity between each layer's input and output during a forward pass.\n",
    "\n",
    "- **Inputs**:  \n",
    "  - `pruned_model`: A model with certain attention layers pruned (dropped).\n",
    "  - `tokenizer`: A tokenizer to process the input text.\n",
    "  - `input_text`: The input text for the model to process.\n",
    "\n",
    "- **Steps**:\n",
    "\n",
    "  1. **Preparation**:\n",
    "     - The model is set to evaluation mode using `eval()` to ensure no gradients are computed.\n",
    "     - The input text is tokenized into tensors suitable for the model.\n",
    "\n",
    "  2. **Identifying Unpruned Layers**:\n",
    "     - A list of unpruned layers is created by checking which layers are not in the `drop_attn_list` (stored in the model’s configuration).\n",
    "  \n",
    "  3. **Creating Hooks for Layer Inputs/Outputs**:\n",
    "     - Two hooks are defined:\n",
    "       - `q_proj_input_hook`: Captures the input to the query projection (`q_proj`) layer.\n",
    "       - `o_proj_output_hook`: Captures the output from the output projection (`o_proj`) layer.\n",
    "     - These hooks are registered for each unpruned layer to capture their inputs and outputs during the forward pass.\n",
    "\n",
    "  4. **Forward Pass**:\n",
    "     - The input is passed through the model using `torch.no_grad()` to avoid computing gradients.\n",
    "     - The hooks capture the inputs and outputs of the unpruned layers.\n",
    "\n",
    "  5. **Removing Hooks**:\n",
    "     - The hooks are removed after the forward pass to prevent memory leaks or interference with further operations.\n",
    "\n",
    "  6. **Computing Importance Scores**:\n",
    "     - For each unpruned layer, the cosine similarity between its input and output is computed.\n",
    "     - **Cosine Similarity**: Measures how similar the input and output are. A high similarity suggests the layer does not significantly transform the input.\n",
    "     - **Importance Score**: Calculated as `1 - similarity`. A higher score indicates the layer plays a more important role in transforming the input.\n",
    "\n",
    "- **Output**:\n",
    "  - A list of tuples containing the layer index and its corresponding importance score.\n",
    "  - Each layer’s importance score is printed as `Layer {idx} importance score: {importance_score:.4f}`.\n",
    "\n",
    "- **Use Case**:  \n",
    "  This function helps to evaluate which unpruned layers contribute significantly to the model's output. Layers with higher importance scores are more critical to the model's decision-making process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d4jGSXw-yGYs"
   },
   "outputs": [],
   "source": [
    "def measure_unpruned_layer_importances(pruned_model, tokenizer, input_text):\n",
    "    \"\"\"\n",
    "    Measures and returns importance scores for all unpruned (non-bypassed) layers.\n",
    "    \"\"\"\n",
    "    # PREPARATION\n",
    "    \"\"\"\n",
    "    set the model to evaluation mode to ensure that no gradients\n",
    "    are computed during the forward pass.\n",
    "    \"\"\"\n",
    "    pruned_model.eval()\n",
    "    device = next(pruned_model.parameters()).device\n",
    "\n",
    "    \"\"\"\n",
    "    The provided input text (input_text) is tokenized into tensors\n",
    "    suitable for processing by the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    \"\"\"This will hold tuples of (layer_idx, importance_score)\"\"\"\n",
    "    importance_scores = []\n",
    "\n",
    "    # IDENTIFY UNPRUNED LAYERS & CREATING HOOKS\n",
    "    \"\"\"\n",
    "    We'll register hooks for only layers that are NOT in drop_attn_list\n",
    "    The list of attention layers that have already been pruned,\n",
    "    is stored in a variable in the model's config: pruned_model.config.drop_attn_list.\n",
    "    \"\"\"\n",
    "    unpruned_layer_indices = [\n",
    "        idx for idx in range(len(pruned_model.model.layers))\n",
    "        if idx not in pruned_model.config.drop_attn_list\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    Temporary storage for each layer's input/output\n",
    "    We'll store them by layer index\n",
    "    \"\"\"\n",
    "    layer_inputs = {}\n",
    "    layer_outputs = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Create 2 hooks to capture the input and the output of the layers.\n",
    "    These hooks store the inputs and outputs in dictionaries\n",
    "    (layer_inputs and layer_outputs) for later analysis\n",
    "    \"\"\"\n",
    "    #Allows capture the input to the query projection (q_proj)\n",
    "    def q_proj_input_hook(layer_idx):\n",
    "        def _hook(module, module_input):\n",
    "            # module_input can be a tuple depending on PyTorch version\n",
    "            inp = module_input[0] if isinstance(module_input, tuple) else module_input\n",
    "            layer_inputs[layer_idx] = inp.detach().clone()\n",
    "        return _hook\n",
    "\n",
    "    # Allows capture the output from the output projection (o_proj)\n",
    "    def o_proj_output_hook(layer_idx):\n",
    "        def _hook(module, module_input, module_output):\n",
    "            out = module_output[0] if isinstance(module_output, tuple) else module_output\n",
    "            layer_outputs[layer_idx] = out.detach().clone()\n",
    "        return _hook\n",
    "\n",
    "    # Register hooks for each unpruned layer\n",
    "    handles = []\n",
    "    for idx in unpruned_layer_indices:\n",
    "        layer = pruned_model.model.layers[idx]\n",
    "        handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n",
    "        handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n",
    "\n",
    "    # FORWARD PASS\n",
    "    \"\"\"\n",
    "    Single forward pass (no gradient needed)\n",
    "    A single forward pass is performed on the input text.\n",
    "    During this pass, the hooks capture the inputs and outputs of the unpruned layers.\n",
    "    This step is done with torch.no_grad(),\n",
    "    ensuring no gradients are calculated, which saves memory and computation.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        _ = pruned_model(**inputs)\n",
    "\n",
    "    \"\"\"\n",
    "    The hooks are removed after the forward pass\n",
    "    to avoid memory leaks or interference with subsequent operations.\n",
    "    \"\"\"\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "\n",
    "    #COMPUTE IMPORTANCE SCORES\n",
    "    \"\"\"\n",
    "    For each unpruned layer, the inputs and outputs are flattened into vectors for comparison.\n",
    "\n",
    "    Cosine Similarity: The similarity between the input and output vectors is\n",
    "    computed using cosine similarity. Layers with outputs that are very similar\n",
    "    to their inputs likely contribute less to the model’s overall computation.\n",
    "\n",
    "    Importance Score: The importance score for each layer is calculated as 1−similarity\n",
    "    A higher score indicates that the layer transforms its input significantly\n",
    "    and is therefore more important to the model's function.\n",
    "    \"\"\"\n",
    "    for idx in unpruned_layer_indices:\n",
    "        if idx in layer_inputs and idx in layer_outputs:\n",
    "            inp = layer_inputs[idx]\n",
    "            out = layer_outputs[idx]\n",
    "\n",
    "            inp_flat = inp.view(inp.size(0), -1)\n",
    "            out_flat = out.view(out.size(0), -1)\n",
    "\n",
    "            similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n",
    "            importance_score = 1 - similarity\n",
    "            importance_scores.append((idx, importance_score))\n",
    "\n",
    "            print(f\"Layer {idx} importance score: {importance_score:.4f}\")\n",
    "\n",
    "    \"\"\"A list of tuples is returned, where each tuple contains the layer index\n",
    "    and its calculated importance score.\"\"\"\n",
    "    return importance_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8M8su2qcjg2"
   },
   "source": [
    "The function `bypass_single_layer` is used to disable the attention mechanism of a specific layer in the model without permanently removing or modifying the layer.\n",
    "\n",
    "This is achieved by dynamically overriding the layer’s forward method to bypass its attention computation.\n",
    "\n",
    "As the attention layers are grouped with the MLP Layers we can just remove an attention layer without removing the associated MLP layer. But we can bypass the layer.\n",
    "\n",
    "The bypassed layer skips computationally expensive attention operations, reducing inference time and memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Bypassing a Single Attention Layer in a Pruned Model\n",
    "\n",
    "- **Function Purpose**:  \n",
    "  The `bypass_single_layer` function modifies a specified attention layer in a pruned model to bypass (skip) the layer's computation during the forward pass.\n",
    "\n",
    "- **Inputs**:  \n",
    "  - `pruned_model`: The pruned model that has certain layers removed.\n",
    "  - `layer_idx`: The index of the attention layer to bypass.\n",
    "\n",
    "- **Steps**:\n",
    "\n",
    "  1. **Accessing the Layer**:\n",
    "     - The layer at the specified `layer_idx` is accessed from the model’s layers.\n",
    "\n",
    "  2. **Bypass Flag Setup**:\n",
    "     - The list `drop_attn_list` from the model's configuration is retrieved, which holds indices of layers to be skipped.\n",
    "     - A `layer_idx` is assigned to the layer’s attention submodule to track its position.\n",
    "\n",
    "  3. **Store Original Forward Method**:\n",
    "     - If the `self_attn` (self-attention) module does not already have a stored original forward method, it is saved as `_original_forward`. This ensures we can return to the original behavior later.\n",
    "\n",
    "  4. **Set Bypass Flag**:\n",
    "     - A custom attribute `_should_bypass` is added to the attention layer and set to `True`. This flag will control whether the layer is bypassed.\n",
    "\n",
    "  5. **Define New Forward Method**:\n",
    "     - A new `new_attention_forward` method is defined that:\n",
    "       - Checks if the layer's index is in the `drop_attn_list`.\n",
    "       - If it is, it returns the hidden states without applying the attention mechanism (bypassing the layer).\n",
    "       - Otherwise, it calls the original forward method (`_orig_forward`) to process the attention normally.\n",
    "\n",
    "  6. **Set New Forward Method**:\n",
    "     - The layer’s attention module's forward method is replaced with the newly defined `new_attention_forward`.\n",
    "\n",
    "- **Output**:\n",
    "  - The specified attention layer is now bypassed during the forward pass of the model, meaning its computations are skipped.\n",
    "\n",
    "- **Use Case**:  \n",
    "  This function is useful in pruning experiments or optimizing models, where certain layers (e.g., attention layers) can be skipped to reduce computational overhead or analyze the effects of layer removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yblvEOSgyKd5"
   },
   "outputs": [],
   "source": [
    "def bypass_single_layer(pruned_model, layer_idx):\n",
    "    \"\"\"\n",
    "    Modifies the specified layer's forward method so that attention is bypassed.\n",
    "    \"\"\"\n",
    "    layer = pruned_model.model.layers[layer_idx]\n",
    "\n",
    "    # get the list once, while we still have access to the full config\n",
    "    skip = pruned_model.config.drop_attn_list\n",
    "    layer.self_attn.layer_idx = layer_idx\n",
    "\n",
    "    # Store the original forward.\n",
    "    if not hasattr(layer.self_attn, '_original_forward'):\n",
    "        layer.self_attn._original_forward = layer.self_attn.forward\n",
    "\n",
    "    # Set a simple bypass flag directly on the attention layer\n",
    "    layer.self_attn._should_bypass = True\n",
    "\n",
    "    # A new forward that checks the bypass flag\n",
    "    def new_attention_forward(attn, hidden_states, attention_mask=None, position_ids=None,\n",
    "                    past_key_value=None, output_attentions=False, use_cache=False,\n",
    "                    **kwargs):\n",
    "        if attn.layer_idx in skip:\n",
    "            # short-circuit the pruned layer\n",
    "            return (hidden_states, None) if use_cache else (hidden_states, None)\n",
    "        return attn._orig_forward(hidden_states, attention_mask, position_ids,\n",
    "                                  past_key_value, output_attentions, use_cache,\n",
    "                                  **kwargs)\n",
    "    #set new forward method\n",
    "    layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn,\n",
    "                                                  type(layer.self_attn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: One-Shot Pruning of Attention Layers in a Model\n",
    "\n",
    "- **Function Purpose**:  \n",
    "  The `one_shot_pruning_inplace` function performs pruning on a model by bypassing (removing) the least important attention layers in a single step, without creating a copy of the model.\n",
    "\n",
    "- **Inputs**:  \n",
    "  - `model`: The model to be pruned.\n",
    "  - `tokenizer`: Tokenizer for processing the input text.\n",
    "  - `input_text`: The input text used to measure the importance of attention layers.\n",
    "  - `num_layers_to_prune`: The number of attention layers to prune (bypass).\n",
    "\n",
    "- **Steps**:\n",
    "\n",
    "  1. **Save Device**:\n",
    "     - The device (CPU/GPU) of the model is saved for later use.\n",
    "  \n",
    "  2. **Set Up Pruning List**:\n",
    "     - If the model does not have a `drop_attn_list` attribute in its configuration, it is initialized as an empty list. This list will track the indices of pruned layers.\n",
    "\n",
    "  3. **Measure Layer Importance**:\n",
    "     - The function `measure_unpruned_layer_importances` is called to compute the importance scores of all unpruned layers.\n",
    "  \n",
    "  4. **Check for Enough Layers to Prune**:\n",
    "     - If the requested number of layers to prune (`num_layers_to_prune`) exceeds the number of layers with importance scores, an error is raised.\n",
    "\n",
    "  5. **Sort and Select Layers to Bypass**:\n",
    "     - The layers are sorted by their importance scores in ascending order.\n",
    "     - The least important layers (with the lowest scores) are selected to be pruned.\n",
    "\n",
    "  6. **Bypass Selected Layers**:\n",
    "     - For each layer selected to be pruned:\n",
    "       - The layer's index is added to the `drop_attn_list`.\n",
    "       - The `bypass_single_layer` function is called to modify the layer's forward method, effectively bypassing it during the forward pass.\n",
    "       - A message is printed indicating the layer that was bypassed and its importance score.\n",
    "\n",
    "  7. **Print Bypassed Layers**:\n",
    "     - The final list of bypassed layers is printed in ascending order of their indices.\n",
    "\n",
    "- **Output**:\n",
    "  - The modified model with the selected attention layers bypassed (pruned).\n",
    "  \n",
    "- **Use Case**:  \n",
    "  This function is useful for one-shot pruning, where a fixed number of the least important attention layers are removed from a model to improve efficiency or reduce computation while maintaining performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5xf5Uq6YJVm_"
   },
   "outputs": [],
   "source": [
    "def one_shot_pruning_inplace(model, tokenizer, input_text, num_layers_to_prune):\n",
    "    \"\"\"\n",
    "    Performs pruning on the original model without creating a copy.\n",
    "    \"\"\"\n",
    "    # Save original device (should be CPU in your case)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Set up pruning list\n",
    "    if not hasattr(model.config, 'drop_attn_list'):\n",
    "        model.config.drop_attn_list = []\n",
    "\n",
    "    # Measure importance\n",
    "    scores = measure_unpruned_layer_importances(model, tokenizer, input_text)\n",
    "\n",
    "    if len(scores) < num_layers_to_prune:\n",
    "        raise ValueError(\"Requested more layers to prune than exist\")\n",
    "\n",
    "    # Sort and select layers to bypass\n",
    "    scores.sort(key=lambda x: x[1])  # ascending\n",
    "    layers_to_bypass = [idx for idx, _ in scores[:num_layers_to_prune]]  # Fixed syntax error\n",
    "\n",
    "    # Bypass selected layers\n",
    "    for idx in layers_to_bypass:\n",
    "        model.config.drop_attn_list.append(idx)\n",
    "        bypass_single_layer(model, idx)\n",
    "        print(f\"Bypassing layer {idx} with importance score {dict(scores)[idx]:.4f}\")\n",
    "\n",
    "    print(f\"Bypassed layers: {sorted(model.config.drop_attn_list)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOuKyH0MjyJM"
   },
   "source": [
    "## Execute Pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyI5XiqYyBnc"
   },
   "source": [
    "**Disclaimer**\n",
    "\n",
    "I'm using a single illustrative prompt so that the code path is easy to follow. In any research or production setting you must feed hundreds or thousands of diverse prompts before deciding which layers to deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvhSYyE2Rk1H",
    "outputId": "efd0cc2d-b9a2-474e-c415-60a7b487e731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 importance score: 1.0711\n",
      "Layer 1 importance score: 0.9268\n",
      "Layer 2 importance score: 0.9592\n",
      "Layer 3 importance score: 0.9586\n",
      "Layer 4 importance score: 0.9556\n",
      "Layer 5 importance score: 1.0017\n",
      "Layer 6 importance score: 1.0273\n",
      "Layer 7 importance score: 1.0322\n",
      "Layer 8 importance score: 1.1082\n",
      "Layer 9 importance score: 1.1019\n",
      "Layer 10 importance score: 1.0403\n",
      "Layer 11 importance score: 0.9950\n",
      "Layer 12 importance score: 1.0824\n",
      "Layer 13 importance score: 1.0197\n",
      "Layer 14 importance score: 0.9663\n",
      "Layer 15 importance score: 0.9675\n",
      "Layer 16 importance score: 0.9030\n",
      "Layer 17 importance score: 0.9355\n",
      "Layer 18 importance score: 1.0156\n",
      "Layer 19 importance score: 0.7300\n",
      "Layer 20 importance score: 0.8689\n",
      "Layer 21 importance score: 0.9351\n",
      "Layer 22 importance score: 0.8706\n",
      "Layer 23 importance score: 0.7950\n",
      "Layer 24 importance score: 0.8481\n",
      "Layer 25 importance score: 0.8776\n",
      "Layer 26 importance score: 0.7842\n",
      "Layer 27 importance score: 0.9855\n",
      "Bypassing layer 19 with importance score 0.7300\n",
      "Bypassing layer 26 with importance score 0.7842\n",
      "Bypassing layer 23 with importance score 0.7950\n",
      "Bypassing layer 24 with importance score 0.8481\n",
      "Bypassed layers: [19, 23, 24, 26]\n"
     ]
    }
   ],
   "source": [
    "pruned_model = one_shot_pruning_inplace(\n",
    "      model,\n",
    "      tokenizer,\n",
    "       \"Hi I'm a sample text, used to calculate the cosine difference between input and output.\",\n",
    "      num_layers_to_prune=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46_tIN1brKZW"
   },
   "source": [
    "# Test Pruned Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFdCesVcnnrM"
   },
   "source": [
    "Now, let's test the pruned model, which is a Llama-3.2-3B model where I have marked 4 Attention layers to be bypassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oCJ8EYLhJg-",
    "outputId": "e082ce54-3b82-4a53-b802-1eaca0b7c597"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1:\n",
      "Tokenization time: 0.45 ms\n",
      "Generation time: 19553.47 ms\n",
      "Decoding time: 0.29 ms\n",
      "Total time: 19554.41 ms\n",
      "\n",
      "Run 2:\n",
      "Tokenization time: 0.88 ms\n",
      "Generation time: 20109.66 ms\n",
      "Decoding time: 0.35 ms\n",
      "Total time: 20111.05 ms\n",
      "\n",
      "Average time over 2 runs: 19832.55 ms\n",
      "Generated text: ['Paris is the capital of France and also its largest city. It is also one of the most visited tourist destination worldwide with millions of tourists visiting every year. There are many things to do in Paris including sightseeing tours, shopping malls, museums etc', 'Paris is the capital of France and also its largest city. It is also one of the most visited tourist destination worldwide with millions of tourists visiting every year. There are many things to do in Paris including sightseeing tours, shopping malls, museums etc']\n"
     ]
    }
   ],
   "source": [
    "# Test the pruned model\n",
    "pruned_model = pruned_model.to(device) #Move the model to GPU again.\n",
    "generated = get_output(prompt, pruned_model, num_runs=2)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ic4ux7goGu_"
   },
   "source": [
    "\n",
    "The execution of this second model is slightly faster than that of the base model, and the generated text is fairly accurate, although some repetition can be noticed towards the end of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7R_VaIPTQhN"
   },
   "source": [
    "# Store the Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERXYQsy5TUtn",
    "outputId": "23347781-ef0d-41d2-fe76-abf62b5744f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model saved to ./attnprun-llama-3.2-3B\n"
     ]
    }
   ],
   "source": [
    "new_model_name = 'attnprun-llama-3.2-3B'\n",
    "output_dir = './'+new_model_name\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "pruned_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "#new_config.save_pretrained(output_dir)\n",
    "print(f\"Pruned model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdE7o0PETggr",
    "outputId": "8106fbb6-d795-4948-f805-6dd39b511237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_attn_list stored: [19, 26, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "# 2. Check that config contains layers to skip\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "\n",
    "if hasattr(config, \"drop_attn_list\"):\n",
    "    print(f\"drop_attn_list stored: {config.drop_attn_list}\")\n",
    "else:\n",
    "    print(\"drop_attn_list isn't present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG63t8jqVdOx"
   },
   "source": [
    "## Upload to Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRDoj5bYKWVh"
   },
   "source": [
    "El proceso de subida de este modelo a Hugging es ligeramente más complejo por que se debe almacenar no tan solo el modelo en si, sino tambien el código de la función _bypass_single_layer. Que como recordarás es la función que se encarga de decidir cuando ejecutar o simplemente bypasear una capa de atención.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0bi3zX7FVjwp"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder, whoami, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_AnSaJPuWffg",
    "outputId": "78ec43c7-4a31-4e21-be3a-52d6942b6b10"
   },
   "outputs": [],
   "source": [
    "# Step 1: Get your HF username from the current token\n",
    "username = whoami()[\"name\"]  # Returns a dict like {'name': 'your_username', 'email': ...}\n",
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KDuY1p8zWjax"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define repo name\n",
    "repo_id = f\"{username}/{new_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-MNgDvImWmFa"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define path to your model\n",
    "output_dir = \"./\"+new_model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJSb5z2nLRS-"
   },
   "source": [
    "The function must be saved in a .py file, but since this notebook runs on Colab, I’ve decided the best approach is to create a cell that generates the file to be uploaded.\n",
    "\n",
    "The file contains the custom class PrunedLlamaForCausalLM, which extends Hugging Face’s LlamaForCausalLM.\n",
    "\n",
    "This custom class calls the base constructor, ensuring that the model's configuration file includes the drop_attn_list, which specifies the layers that should be skipped.\n",
    "\n",
    "The forward function is modified only for the layers that need to be skipped; the rest continue executing their standard forward function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_xaG0FDXojc",
    "outputId": "4dc0ee1d-c49f-41f9-ddad-76ff0da044f4"
   },
   "outputs": [],
   "source": [
    "custom_model_code = '''\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "class PrunedLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        if not hasattr(config, \"drop_attn_list\"):\n",
    "            config.drop_attn_list = []\n",
    "\n",
    "        for idx in config.drop_attn_list:\n",
    "            self._bypass_single_layer(idx)\n",
    "\n",
    "    def _bypass_single_layer(pruned_model, layer_idx):\n",
    "        \"\"\"\n",
    "        Modifies the specified layer's forward method so that attention is bypassed.\n",
    "        \"\"\"\n",
    "        layer = pruned_model.model.layers[layer_idx]\n",
    "\n",
    "        # get the list once, while we still have access to the full config\n",
    "        skip = pruned_model.config.drop_attn_list\n",
    "        layer.self_attn.layer_idx = layer_idx\n",
    "\n",
    "        # Store the original forward.\n",
    "        if not hasattr(layer.self_attn, '_original_forward'):\n",
    "            layer.self_attn._original_forward = layer.self_attn.forward\n",
    "\n",
    "        # Set a simple bypass flag directly on the attention layer\n",
    "        layer.self_attn._should_bypass = True\n",
    "\n",
    "        # A new forward that checks the bypass flag\n",
    "        def new_attention_forward(attn, hidden_states, attention_mask=None, position_ids=None,\n",
    "                        past_key_value=None, output_attentions=False, use_cache=False,\n",
    "                        **kwargs):\n",
    "            if attn.layer_idx in skip:\n",
    "                # short-circuit the pruned layer\n",
    "                return (hidden_states, None) if use_cache else (hidden_states, None)\n",
    "            return attn._orig_forward(hidden_states, attention_mask, position_ids,\n",
    "                                      past_key_value, output_attentions, use_cache,\n",
    "                                      **kwargs)\n",
    "        #set new forward method\n",
    "        layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn,\n",
    "                                                      type(layer.self_attn))\n",
    "\n",
    "'''\n",
    "\n",
    "# Define path and write the file\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"modeling_attnprun_llama.py\"), \"w\") as f:\n",
    "    f.write(custom_model_code.strip())\n",
    "\n",
    "print(\"Custom model script modeling_attnprun_llama.py created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r59Yae3jVMpc"
   },
   "source": [
    "Now the model's configuration file is updated by adding the `auto_map` field, which tells the Transformers library which class to use to construct the model: `modeling_attnprun_llama.PrunedLlamaForCausalLM.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBbxbBbmbeRl",
    "outputId": "f5b15a92-2770-47c4-a98e-832f68d2d2af"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to the config file\n",
    "config_path = os.path.join(output_dir, \"config.json\")\n",
    "\n",
    "# Load the existing config\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Add or update the auto_map section\n",
    "config[\"auto_map\"] = {\n",
    "    \"AutoModelForCausalLM\": \"modeling_attnprun_llama.PrunedLlamaForCausalLM\"\n",
    "}\n",
    "\n",
    "# Optional: ensure the architecture field is aligned\n",
    "config[\"architectures\"] = [\"PrunedLlamaForCausalLM\"]\n",
    "\n",
    "# Save the updated config\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"config.json updated with auto_map and architecture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ciXxElsWsQm"
   },
   "source": [
    "Time to upload the folder containing the weights, the config file and the new function to HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "TTOCOdMhqzo8",
    "outputId": "db1a98d3-bef7-4c93-e376-7f2286435432"
   },
   "outputs": [],
   "source": [
    "create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCUToiTmWp9M",
    "outputId": "51b81151-6533-4bb6-e070-8d43c1e154e5"
   },
   "outputs": [],
   "source": [
    "# Step 4: Upload the folder to the Hub\n",
    "upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    path_in_repo=\"\",  # Upload everything to root\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"Model uploaded successfully to https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lp3bWMOehLi"
   },
   "source": [
    "## Download model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2eDLdnEmapS",
    "outputId": "a2a18d6e-c62d-463a-e3c3-e9d269bf4251"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del pruned_model\n",
    "del tokenizer\n",
    "del model\n",
    "\n",
    "# 2. Libera la caché de la GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()  # Opcional, ayuda en Colab\n",
    "\n",
    "# 3. Forza recolección de basura en Python\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iy6GqernoZJx",
    "outputId": "8463581c-85c4-4ddf-a2d9-8e22c6466975"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: argument {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}: invalid choice: 'cache' (choose from 'download', 'upload', 'repo-files', 'env', 'login', 'whoami', 'logout', 'auth', 'repo', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache', 'tag', 'version', 'upload-large-folder')\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli cache purge --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgHQ5-2MW6q2"
   },
   "source": [
    "The model is downloaded normally from Hugging Face, but you must remember to set `trust_remote_code=True` since the model includes the custom code you previously created and uploaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "d4AB09pNemii"
   },
   "outputs": [],
   "source": [
    "model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oQArVXsSmtp-"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2feKDCrPll0E",
    "outputId": "be1d7ecc-817d-4229-b8d0-bb011029e55f"
   },
   "outputs": [],
   "source": [
    "model_hf = model_hf.to(device) #Move the model to GPU again.\n",
    "generated = get_output(prompt, model_hf, num_runs=2)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CsaXViPtmft"
   },
   "source": [
    "# Conclusion.\n",
    "Based on the findings in the paper and the results obtained, I believe this type of pruning may work better with larger models where attention layers tend to have redundancy.\n",
    "\n",
    "Since this type of pruning does not alter the model's structure, it does not result in a reduction in its size or the memory required to load it. The main advantage of using this pruning approach is the reduction of computational load during inference, leading to a more efficient model with faster responses and lower resource consumption.\n",
    "\n",
    "Unlike the original paper, which describes \"removing\" selected attention layers but provides limited implementation details, this implementation takes a transparent functional approach by explicitly overriding the `forward` method only in the specified layers. As a result, the model retains its full architecture and parameter set, but selectively skips computations at runtime. This makes the method reversible, modular, and fully compatible with the Hugging Face ecosystem using `trust_remote_code=True`. While both approaches achieve similar computational savings, this one emphasizes clarity, portability, and practical integration.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94s3XHmpJteMPW8MWfPBn",
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
