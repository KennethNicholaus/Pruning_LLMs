{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT0FAsUnncFY"
   },
   "source": [
    "<div>\n",
    "    <h2>Pruning Llama 3.2. with OptiPFair</h2>\n",
    "    <h3>Using OptiPFair to prune MLP Layers with GLU structure.</h3>\n",
    "</div>\n",
    "* Pruning\n",
    "* Structured pruning\n",
    "* optiPfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zHApVm41HWq",
    "outputId": "f542547a-13b6-418d-cf7f-18b1217d7603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from dotenv) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q optipfair\n",
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqSfFAukLZ_J",
    "outputId": "913ee4e9-8fbf-4424-caf8-fb14c97cf724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: optipfair\n",
      "Version: 0.1.4\n",
      "Summary: A library for structured pruning & Bias visualization of large language models\n",
      "Home-page: https://github.com/peremartra/optipfair\n",
      "Author: Pere Martra\n",
      "Author-email: peremartra@uadla.com\n",
      "License: \n",
      "Location: /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages\n",
      "Requires: click, torch, tqdm, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show optipfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GJNgRj4M187E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from optipfair import prune_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbIyUlXEtbqs",
    "outputId": "c707b2da-369c-4522-f3ea-bf61d190516a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM-QwxyKw-YG"
   },
   "source": [
    "#Download model and explore structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18174d9f52f448b8e0a6f2e5820d8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b504a1cc865b4c5ebb9f57bf36738bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591007a525ce4ddb95f84431f0e9085c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776abe456c7a46b3836a1a7ea36e1eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d50920a5da0419ea3ef01fe2f5295e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564a6b45dbf942be83d9eb5cc7341839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration from environment variables\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "model_name = os.getenv('MODEL_NAME', 'meta-llama/Llama-3.2-1B')\n",
    "\n",
    "# Check if HF_TOKEN is provided\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in environment variables. Please add it to your .env file.\")\n",
    "\n",
    "# Load model and tokenizer with authentication token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,\n",
    "    token=hf_token\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9UpMD4Hw2MWg"
   },
   "outputs": [],
   "source": [
    "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        do_sample=False,          # Disable sampling\n",
    "        num_beams=5,              # Use beam search\n",
    "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
    "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4muyx_8M5OAu"
   },
   "source": [
    "## studying the model structure\n",
    "As demonstrated in the [previous notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb), studying the structure of the model that will undergo pruning is crucial.\n",
    "\n",
    "In this notebook, we’re going to fine-tune the pruning process for the Llama3.2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5Hs4oQ4B7Z0",
    "outputId": "d2b81330-b384-452d-934e-f2e0cd096871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPMslK3QCAb1"
   },
   "source": [
    "\n",
    "An MLP block typically consists of layers that scale the data to larger dimensions and others that return it to its original size.\n",
    "\n",
    "In the MLP block of the model, we find two projection layers: `gat_proj` and `down_proj`, both scaling from 2048 to 8192. The purpose of having two layers projecting to the same intermediate size might be related to gating mechanisms. A gating mechanism selectively controls information flow in neural networks by using learned weights to \"gate\" or filter inputs.\n",
    "\n",
    "However, to truly understand how these layers function, we’d need to refer to the model's documentation or even the source code. But, this structure usually indicates, at least, I haven't encountered a case where it doesn't, that the layers performing the upsizing work in pairs, and they cannot be treated as independent linear layers.\n",
    "\n",
    "In other words, any operation we apply to one layer must be replicated in the other. Most importantly, when identifying which neurons have more or less importance, we can't evaluate the neurons of a single layer in isolation; we need to treat them as pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alKH3QH64WFL",
    "outputId": "852b54cb-5537-4d86-a021-29a15ff83768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n"
     ]
    }
   ],
   "source": [
    "# Test the original model\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated = get_output(prompt)\n",
    "print(f\"Generated text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK9NwmBWnkSP"
   },
   "source": [
    "#Pruning the Model.\n",
    "##Support pruning functions.\n",
    "###Compute neuron importance functions.\n",
    "\n",
    "To perform the pruning process, you need to call the `prune_model` function from the library (OptiPFair)[https://github.com/peremartra/optipfair].\n",
    "\n",
    "To use this function, you need to provide the following parameters:\n",
    "\n",
    "* model: The model to be pruned.\n",
    "* pruning_type: The type of pruning to apply. In this case, it will be \"MLP_GLU\", which is currently the only option supported by the library.\n",
    "* neuron_selection_method:\n",
    "  * MAW: Maximum Absolute Weight Values.\n",
    "  * VOW: Variance Of Weigths.\n",
    "  * PON: Product Of Norms.\n",
    "  With LLaMA models, the method that works best without requiring further training is MAW.\n",
    "* pruning_percentage o expansion_rate: you need to provide one of them. In this notebook, we’ll use pruning_percentage.\n",
    "* show_progress: By default, it's set to True. It displays the progress of the pruning process.\n",
    "* return_stats: By default, it's set to True. It returns the percentage of neurons removed and the resulting expansion rate.\n",
    "\n",
    "\n",
    "*I’m leaving the others in the notebook purely as an exercise.*\n",
    "\n",
    "The **MAW** method works better because it directly identifies the most influential neurons based on the magnitude of their connections. These neurons are likely responsible for key decisions, making the model more accurate after pruning. The Variance of Weights method, while useful in some contexts, can retain neurons that may not contribute significantly to the task, leading to less coherent model outputs.\n",
    "\n",
    "However, we shouldn’t fall into the trap of assuming that this neuron selection method will work best across all model structures. It works well with Llama models, and this may be due to several factors:\n",
    "\n",
    "* The relatively large projection from 2048 to 8192.\n",
    "* The use of a GLU structure.\n",
    "* The type of activation function used.\n",
    "\n",
    "So, if we use a model from another family, like Gemma or Mistral, the neuron selection method might need to be entirely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtHtSbRmS267"
   },
   "source": [
    "## Obtain & test the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIUnFU5R3n42",
    "outputId": "5df1c68a-4740-42f5-a34d-f79bc2dafc82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pruning layers: 100%|███████████████████████████████████████████████████████████████████| 16/16 [00:05<00:00,  3.15it/s]\n",
      "2025-09-21 22:17:15,256 - optipfair.pruning.mlp_glu - INFO - Updated model config: intermediate_size = 6554\n"
     ]
    }
   ],
   "source": [
    "# Prune 10% of neurons from MLP layers using MAW method\n",
    "pruned_model, stats = prune_model(\n",
    "    model=model,\n",
    "    pruning_type=\"MLP_GLU\",\n",
    "    neuron_selection_method=\"MAW\",\n",
    "    pruning_percentage=20,\n",
    "    show_progress=True,\n",
    "    return_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdJUkfWI3qMM",
    "outputId": "10fc16d7-0c83-4554-ee1d-e24368d21fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original parameters: 1,235,814,400\n",
      "Pruned parameters: 1,074,792,448\n",
      "Reduction: 161,021,952 parameters (13.03%)\n",
      "Expansion rate: 320.01953125%\n"
     ]
    }
   ],
   "source": [
    "# Print pruning statistics\n",
    "print(f\"Original parameters: {stats['original_parameters']:,}\")\n",
    "print(f\"Pruned parameters: {stats['pruned_parameters']:,}\")\n",
    "print(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n",
    "print(f\"Expansion rate: {stats['expansion_rate']:,}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_Zc5VM86KTQ",
    "outputId": "b1e1b858-6913-46ee-97ea-6c8acfabea49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (pruned model): Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are some things you\n"
     ]
    }
   ],
   "source": [
    "# prompt: call the generate_response for the pruned_model\n",
    "\n",
    "# Assuming 'pruned_model' is defined as in the provided code.\n",
    "# Assuming 'get_output' function is also defined as in the provided code.\n",
    "\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated = get_output(prompt, model=pruned_model) # Pass the pruned model to the get_output function\n",
    "print(f\"Generated text (pruned model): {generated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGzXMQrVTULv"
   },
   "source": [
    "The result is slightly different from what the original model produced, but it’s still a fairly accurate response.\n",
    "\n",
    "In contrast to the model created in notebook: [6_2_pruning_structured_llama3.2-1b_KO.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb) where the pruned Llama model lost almost all its utility, the model in this notebook retains a good portion of its knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDQrSrf-VCyI"
   },
   "source": [
    "Looking at the model’s new structure, we can see that the `gate_proj` and `up_proj` layers have had their `out_features` reduced to 6554 from 8192. Consequently, the `down_proj` layer has its `in_features` adjusted to match the new size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATAiqZW30NYN",
    "outputId": "b0b4b451-8ef6-409e-ad84-9a6c8fbdc3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=6554, bias=False)\n",
      "          (down_proj): Linear(in_features=6554, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S27hrDroU04D"
   },
   "outputs": [],
   "source": [
    "from optipfair.evaluation.benchmarks import time_inference, compare_models_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsFAAAsrU3dQ",
    "outputId": "c7121c20-7112-4837-ecb4-5ab4777d74b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 22:17:18,802 - optipfair.evaluation.benchmarks - INFO - Testing prompt: Paris is the capital of...\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-09-21 22:17:36,723 - optipfair.evaluation.benchmarks - INFO - Testing prompt: The speed of light is approximately...\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Compare inference speed\n",
    "comparison = compare_models_inference(\n",
    "    model,\n",
    "    pruned_model,\n",
    "    tokenizer,\n",
    "    prompts=[\"Paris is the capital of\", \"The speed of light is approximately\"],\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4ExKBrBVGW7",
    "outputId": "36b574fb-4c99-403c-a21f-f61d12e6db2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_original_time': 1.222599744796753,\n",
       " 'avg_pruned_time': 1.3710638761520386,\n",
       " 'avg_original_tokens_per_second': 40.89978016969947,\n",
       " 'avg_pruned_tokens_per_second': 36.53589770626485,\n",
       " 'speedup': 0.8917161089737424,\n",
       " 'tps_improvement_percent': -10.669696622642476,\n",
       " 'num_prompts': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
